import pandas as pd

from pyspark.sql import SparkSession
from pyspark.mllib.stat import Statistics
from pyspark.ml import Pipeline
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.feature import IndexToString, StringIndexer, VectorAssembler, OneHotEncoder
from pyspark.ml.evaluation import MulticlassClassificationEvaluator


def loadMongoRDD(db, collection):
    '''
    Download data from mongodb and store it in RDD format
    '''

    spark = SparkSession \
        .builder \
        .master(f"local[*]") \
        .appName("myApp") \
        .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \
        .getOrCreate()

    dataRDD = spark.read.format("mongo") \
        .option('uri', f"mongodb://10.4.41.48/{db}.{collection}") \
        .load() \
        .rdd \
        .cache()

    return dataRDD, spark

def loadMongoDF(db, collection):
    '''
    Download data from mongodb and store it in DF format
    '''
    spark = SparkSession \
        .builder \
        .master(f"local[*]") \
        .appName("myApp") \
        .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \
        .getOrCreate()

    dataDF = spark.read.format("mongo") \
        .option('uri', f"mongodb://10.4.41.48/{db}.{collection}") \
        .load()

    return dataDF, spark


def uploadDFtoMongo(kpi1DF, kpi2DF):
    '''
    Upload the final KPI data to mongodb
    '''
    kpi1DF.write \
        .format("com.mongodb.spark.sql.DefaultSource") \
        .mode("overwrite") \
        .option('uri', f"mongodb://10.4.41.48/exploitation.kpi1") \
        .save()

    kpi2DF.write \
        .format("com.mongodb.spark.sql.DefaultSource") \
        .mode("overwrite") \
        .option('uri', f"mongodb://10.4.41.48/exploitation.kpi2") \
        .save()


def mean(x, var):
    suma = 0
    num = len(x)
    for i in range(0,num):
        suma = suma + x[i][var]
    mean = suma/num

    return float("{:.2f}".format(mean))


def counter(x):
    counter = 0
    num = len(x)
    for i in range(0,num):
        counter = counter + 1

    return counter


def generateKPIs():
    '''
    Loads data from Formatted Zone and performs KPI calculations and 
    trains predictive model, then outputs results to Exploitation Zone in MongoDB.
    '''
    # loading MongoDB 'nested_data' collection as pyspark DataFrame
    rdd, spark = loadMongoRDD(db='formatted', collection='nested_data')

    ## ---------- Desriptive KPI 1 ----------
    ## to know the average of price asked for a flat/apartment per neighborhood + num apartaments to be rent/saled
    rdd1 = rdd.map(lambda x: (x['Neighborhood'], mean(x['Info Idealista'], 'price'), counter(x['Info Idealista'])))
    #rdd1.foreach(lambda r: print(r))
    kpi1DF = rdd1.toDF(['neighborhood', 'average_price', 'n_listings'])

    ## ---------- Desriptive KPI 2 ----------
    ## correlación entre monthly price and RFD (family income index)
    rdd2 = rdd.map(lambda x: (x['Monthly Price (€/month)'], x['RFD most recent']))
    #print(Statistics.corr(rdd2, method="pearson")) #corr of 0.98
    #kpi2DF = spark.createDataFrame(pd.DataFrame(Statistics.corr(rdd2, method="pearson"), columns=['monthly_price_corr', 'RFD_corr']))
    kpi2DF = rdd2.toDF(['Monthly Price (€/month)', 'RFD most recent'])

    # Upload Descriptive KPIs to mongoDB 
    uploadDFtoMongo(kpi1DF, kpi2DF)

    ## ---------- Predictive KPI 3 ----------
    ## modelling --> predict number of rooms given a price and neigbirhood_id
    dataDF, spark = loadMongoDF(db='formatted', collection='data')
    subsetDF = dataDF.select('Neighborhood Id', 'Price', 'Rooms') \
        .withColumnRenamed("Neighborhood Id","Neighborhood_ID")

    # creating label index from 'Rooms' and Feature Vector from 'features'
    labelIndexer = StringIndexer(inputCol="Rooms", outputCol="indexedRooms").fit(subsetDF).setHandleInvalid("keep")

    # Indexing 'Neighborhood_ID' (necessary for one-hot encoding)
    indexers = [StringIndexer(inputCol=column, outputCol=column+"_INDEX").fit(subsetDF).setHandleInvalid("keep") for column in ['Neighborhood_ID']]

    # One-Hot Encoding 'Neighborhood_ID'
    single_col_ohe = OneHotEncoder(inputCol="Neighborhood_ID_INDEX", outputCol="Neighborhood_ID_OneHot")

    # assembling feature vector for model
    assembler = VectorAssembler(inputCols=['Neighborhood_ID_OneHot', 'Price'], outputCol="indexedFeatures")

    # Split the data into training and test sets (30% held out for testing)
    (trainingDataDF, testDataDF) = subsetDF.randomSplit([0.7, 0.3])

    # Train a RandomForest model.
    rf = RandomForestClassifier(labelCol="indexedRooms",
                                featuresCol="indexedFeatures",
                                numTrees=3,
                                maxDepth=4,
                                maxBins=32)

    # Convert indexed labels back to original labels.
    labelConverter = IndexToString(inputCol="prediction", outputCol="predictedLabel", labels=labelIndexer.labels)

    # Chain indexers and forest in a Pipeline
    pipeline = Pipeline().setStages([labelIndexer] + indexers + [single_col_ohe, assembler, rf])

    # Train model.  This also runs the indexers.
    model = pipeline.fit(trainingDataDF)

    # Make predictions.
    predictions = model.transform(testDataDF)

    # Select (prediction, true label) and compute test error
    evaluator = MulticlassClassificationEvaluator(
        labelCol="indexedRooms", predictionCol="prediction", metricName="accuracy")
    accuracy = evaluator.evaluate(predictions)
    print("Test Error = %g" % (1.0 - accuracy)) #0.614

    rfModel = model.stages
    print(rfModel)  # summary only
    model.write().overwrite().save('exploitation/modelRF')


if __name__ == '__main__':
    generateKPIs()
